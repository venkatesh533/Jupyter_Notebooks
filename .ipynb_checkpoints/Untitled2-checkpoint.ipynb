{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48937b3a-a442-41ed-a396-08855cadbfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col,lit,max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6754407b-1c1b-4b7e-b44d-3d4f33e19117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/01/31 19:06:06 WARN Utils: Your hostname, venkatesh-VirtualBox resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "24/01/31 19:06:06 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/01/31 19:06:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('notebook').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e352d02-8a77-4b0d-a2db-82f8ec8fbf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample dataset creation\n",
    "data = [(\"AA\", \"John\", 5),\n",
    "        (\"BB\", \"Alice\", 8),\n",
    "        (\"CC\", \"Bob\", 12),\n",
    "        (\"AA\", \"Eve\", 4),\n",
    "        (\"DD\", \"Charlie\", 7)]\n",
    "columns = [\"Brand\", \"Name\", \"Rank\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2d21cae-45f4-4cb2-97e6-9de74bccbb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(data, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bcfc499-f1b2-4a60-a14b-5f36ccd36b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+----+\n",
      "|Brand|   Name|Rank|\n",
      "+-----+-------+----+\n",
      "|   AA|   John|   5|\n",
      "|   BB|  Alice|   8|\n",
      "|   CC|    Bob|  12|\n",
      "|   AA|    Eve|   4|\n",
      "|   DD|Charlie|   7|\n",
      "+-----+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Original dataset')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e8f6e85-68a8-4a30-88f8-796ed913d1ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered dataset rank <= 8\n",
      "+-----+-------+----+\n",
      "|Brand|   Name|Rank|\n",
      "+-----+-------+----+\n",
      "|   AA|   John|   5|\n",
      "|   BB|  Alice|   8|\n",
      "|   AA|    Eve|   4|\n",
      "|   DD|Charlie|   7|\n",
      "+-----+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_df = df.filter(df['Rank'] <= 8)\n",
    "print('filtered dataset rank <= 8')\n",
    "filtered_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc482487-7975-4d27-80f7-efbefe67afad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+----+\n",
      "|Brand|   Name|Rank|\n",
      "+-----+-------+----+\n",
      "|   DD|Charlie|   7|\n",
      "+-----+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check whether \"AA\", \"BB\", \"CC\" are present in the filtered dataset\n",
    "missing_brands = [\"AA\", \"BB\", \"CC\"]\n",
    "missing_brands_df = filtered_df.filter(~col(\"Brand\").isin(missing_brands))\n",
    "missing_brands_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f6f7fa0-b915-4312-9252-2a3444963252",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Union can only be performed on tables with the same number of columns, but the first table has 3 columns and the second table has 4 columns;\n'Union false, false\n:- Project [CASE WHEN (Brand#0 = AA) THEN cast(AA as string) WHEN (Brand#0 = BB) THEN cast(AA as string) WHEN (Brand#0 = CC) THEN cast(AA as string) ELSE Brand#0 END AS Brand#86, Name#1, Rank#2L]\n:  +- Filter (Rank#2L <= cast(8 as bigint))\n:     +- LogicalRDD [Brand#0, Name#1, Rank#2L], false\n+- Project [Name#78, Rank#79, Name#100, Rank#101L]\n   +- Join RightOuter, (Brand#77 = Brand#99)\n      :- LogicalRDD [Brand#77, Name#78, Rank#79], false\n      +- Filter NOT Brand#99 IN (AA,BB,CC)\n         +- Filter (Rank#101L <= cast(8 as bigint))\n            +- LogicalRDD [Brand#99, Name#100, Rank#101L], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# Replace the rows with all columns of the missing brand\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     replaced_df \u001b[38;5;241m=\u001b[39m filtered_df\u001b[38;5;241m.\u001b[39mreplace(missing_brands, [replacement_brand]\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(missing_brands), subset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBrand\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m     replaced_df \u001b[38;5;241m=\u001b[39m \u001b[43mreplaced_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplacement_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmissing_brands_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBrand\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mright_outer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                                      \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmissing_brands_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBrand\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     replaced_df \u001b[38;5;241m=\u001b[39m replaced_df\u001b[38;5;241m.\u001b[39mfillna(replacement_df\u001b[38;5;241m.\u001b[39mfilter(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBrand\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m replacement_brand)\u001b[38;5;241m.\u001b[39mfirst())\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/spark-env/lib/python3.9/site-packages/pyspark/sql/dataframe.py:2257\u001b[0m, in \u001b[0;36mDataFrame.union\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   2247\u001b[0m \u001b[38;5;129m@since\u001b[39m(\u001b[38;5;241m2.0\u001b[39m)\n\u001b[1;32m   2248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munion\u001b[39m(\u001b[38;5;28mself\u001b[39m, other: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   2249\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a new :class:`DataFrame` containing union of rows in this and another\u001b[39;00m\n\u001b[1;32m   2250\u001b[0m \u001b[38;5;124;03m    :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   2251\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2255\u001b[0m \u001b[38;5;124;03m    Also as standard in SQL, this function resolves columns by position (not by name).\u001b[39;00m\n\u001b[1;32m   2256\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2257\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m~/spark-env/lib/python3.9/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/spark-env/lib/python3.9/site-packages/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Union can only be performed on tables with the same number of columns, but the first table has 3 columns and the second table has 4 columns;\n'Union false, false\n:- Project [CASE WHEN (Brand#0 = AA) THEN cast(AA as string) WHEN (Brand#0 = BB) THEN cast(AA as string) WHEN (Brand#0 = CC) THEN cast(AA as string) ELSE Brand#0 END AS Brand#86, Name#1, Rank#2L]\n:  +- Filter (Rank#2L <= cast(8 as bigint))\n:     +- LogicalRDD [Brand#0, Name#1, Rank#2L], false\n+- Project [Name#78, Rank#79, Name#100, Rank#101L]\n   +- Join RightOuter, (Brand#77 = Brand#99)\n      :- LogicalRDD [Brand#77, Name#78, Rank#79], false\n      +- Filter NOT Brand#99 IN (AA,BB,CC)\n         +- Filter (Rank#101L <= cast(8 as bigint))\n            +- LogicalRDD [Brand#99, Name#100, Rank#101L], false\n"
     ]
    }
   ],
   "source": [
    "# Replace the rows with all columns of the missing brands\n",
    "if missing_brands_df.count() > 0:\n",
    "    # Select one of the missing brands\n",
    "    replacement_brand = missing_brands[0]\n",
    "    \n",
    "    # Create a DataFrame with the missing brand's values\n",
    "    replacement_df = spark.createDataFrame([(replacement_brand, '', '')], columns)\n",
    "    \n",
    "    # Replace the rows with all columns of the missing brand\n",
    "    replaced_df = filtered_df.replace(missing_brands, [replacement_brand]*len(missing_brands), subset=\"Brand\")\n",
    "    replaced_df = replaced_df.union(replacement_df.join(missing_brands_df, \"Brand\", \"right_outer\")\n",
    "                                      .drop(missing_brands_df.Brand))\n",
    "    replaced_df = replaced_df.fillna(replacement_df.filter(col(\"Brand\") == replacement_brand).first())\n",
    " \n",
    "else:\n",
    "    replaced_df = filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6558cea-8392-4507-be82-b75e9a462751",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
